{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONS (used for finding the best way to save the chat to memory after each session)\n",
    "# \"whole_chat\" or \"individual_messages\" or \"pairs\"\n",
    "save_chat_to_memory_method = \"whole_chat\" \n",
    "# save_chat_to_memory_method = \"individual_messages\"\n",
    "# save_chat_to_memory_method = \"pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index (aka create a database for vectors)\n",
    "index = faiss.IndexFlatL2(384) #384 is the size of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding model and corresponding tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "stored_memory = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create log file for debugging\n",
    "log_file = open(\"log.txt\", \"w\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenise plaintext (aka slice up text in small pieces and convert those small pieces to numerical data)\n",
    "def get_tokenised_sections(texts):\n",
    "    tokenised_sections = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    return tokenised_sections\n",
    "\n",
    "\n",
    "#tokenise each text in the array of texts in batches of 16\n",
    "# def get_tokenised_sections(texts):\n",
    "#     tokenised_sections = []\n",
    "#     for i in range(0, len(texts), 16):\n",
    "#         batch = texts[i:i+16]\n",
    "#         tokenised_batch = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "#         tokenised_sections.append(tokenised_batch)\n",
    "#     return tokenised_sections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the texts using the tokens (aka convert the numerical data to vectors that represent the semantics of the text)\n",
    "def get_embeddings(tokenised_sections):\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**tokenised_sections)\n",
    "        # Use the embedding of the [CLS] token (first token) for each input\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# def get_embeddings(tokenised_sections):\n",
    "#     embeddings = []\n",
    "#     for section in tokenised_sections:\n",
    "#         with torch.no_grad():\n",
    "#             model_output = model(**section)\n",
    "#             embeddings.append(model_output.last_hidden_state[:,0,:].detach().cpu().numpy())\n",
    "#     return torch.cat(embeddings).numpy()\n",
    "\n",
    "\n",
    "# def get_embeddings(texts):\n",
    "#     tokenised_sections = get_tokenised_sections(texts)\n",
    "#     embeddings = []\n",
    "#     for tokenised_batch in tokenised_sections:\n",
    "#         with torch.no_grad():\n",
    "#             model_output = model(**tokenised_batch)\n",
    "#             embeddings.append(model_output.last_hidden_state[:,0,:].detach().cpu().numpy())\n",
    "#     return torch.cat(embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test different things: test memory of whole chat dialog vs memory of each individual message vs memory of pairs of question and answer\n",
    "#  \n",
    "#putting whole chat as a memory entry\n",
    "def add_to_index_wholechat(chat):\n",
    "    combined_chat = \"\"\n",
    "    for i, message in enumerate(chat):\n",
    "        if i % 2 == 0: #then it is from the agent\n",
    "            combined_chat += \"<START OF AGENT MESSAGE>\" + message + \"<END OF AGENT MESSAGE>\"\n",
    "        else:\n",
    "            combined_chat += \"<START OF USER MESSAGE>\" + message + \"<END OF USER MESSAGE>\"\n",
    "        \n",
    "    tokenised_chat = get_tokenised_sections([combined_chat])\n",
    "    embeddings = get_embeddings(tokenised_chat)\n",
    "    index.add(embeddings)\n",
    "    stored_memory.append(combined_chat)\n",
    "\n",
    "#putting each individual message as a memory entry\n",
    "def add_to_index_individual_messages(chat):\n",
    "    for i, message in enumerate(chat):\n",
    "        if i % 2 == 0:\n",
    "            tokenised_message = get_tokenised_sections([\"<START OF AGENT MESSAGE>\" + message + \"<END OF AGENT MESSAGE>\"])\n",
    "        else:\n",
    "            tokenised_message = get_tokenised_sections([\"<START OF USER MESSAGE>\" + message + \"<END OF USER MESSAGE>\"])\n",
    "        embeddings = get_embeddings(tokenised_message)\n",
    "        index.add(embeddings)\n",
    "        stored_memory.append(tokenised_message)\n",
    "\n",
    "#putting each pair of user message and agent message as a memory entry\n",
    "def add_to_index_pairs(chat):\n",
    "    for i in range(1, len(chat), 2):\n",
    "        message_pair = [\"<START OF USER MESSAGE>\" + chat[i-1] + \"<END OF USER MESSAGE>\", \"<START OF AGENT MESSAGE>\" + chat[i] + \"<END OF AGENT MESSAGE>\"]\n",
    "        tokenised_pair = get_tokenised_sections(message_pair)\n",
    "        embeddings = get_embeddings(tokenised_pair)\n",
    "        index.add(embeddings)\n",
    "        stored_memory.append(message_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add individual messages to index (can be used for metadata or important things the user wants the bot to remember for the current session.\n",
    "#  this includes instructions for the session, user preferences, etc.)\n",
    "def add_to_index(data):\n",
    "    tokenised_data = get_tokenised_sections([data])\n",
    "    embeddings = get_embeddings(tokenised_data)\n",
    "    index.add(embeddings)\n",
    "    stored_memory.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity search\n",
    "def search(query, k):\n",
    "    tokenised_query = get_tokenised_sections([query])\n",
    "    query_embedding = get_embeddings(tokenised_query)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tool that can be called by the llm to fetch data from memory\n",
    "@tool\n",
    "def fetch_From_Memory(query, k=3):\n",
    "    \"\"\"\n",
    "    Fetch data from memory that can be used to generate a response to the user\n",
    "    query: any string you think will have the highest similarity to the data you want to fetch. THIS SHOULD BE AS INFORMATIVE AS POSSIBLE TO GET THE BEST RESULTS\n",
    "    k: the number of entries you want to fetch. BEST IS TO KEEP BELOW 5. \n",
    "    return: the data that has the highest similarity to the query\n",
    "    \"\"\"\n",
    "    \n",
    "    #test if k is a number\n",
    "    int_k = 0\n",
    "    try:\n",
    "        int_k = int(k)\n",
    "    except:\n",
    "        return \"Please enter a valid number for k\"\n",
    "    \n",
    "    D, I = search(query, int_k)\n",
    "    # if len(I) > 0 and I[0][0] != -1:\n",
    "    #     stored_data = \"Previously stored information: \" + str(I[0])  # Convert memory to readable format\n",
    "    #     detokenised_data = tokenizer.decode(stored_data)\n",
    "    #     return detokenised_data\n",
    "    information = \"\"\n",
    "    # if len(I) > 0 and I[0][0] != -1:\n",
    "    #     # Retrieve stored text (assuming you stored them in a list)\n",
    "    #     stored_text = stored_memory[I[0][0]]  # Map index back to original text\n",
    "    #     return f\"Previously stored information: {stored_text}\"\n",
    "\n",
    "    #add all the 10 most similar entries to the response\n",
    "    for i in range(int_k):\n",
    "        if I[0][i] != -1:\n",
    "            information += stored_memory[I[0][i]] + \"\\n\"\n",
    "        \n",
    "    if information == \"\":\n",
    "        return \"No information found\"\n",
    "    \n",
    "    #write to log file\n",
    "    log_file.write(\"Query: \" + query + \"\\n\")\n",
    "    log_file.write(\"Information: \" + information + \"\\n\")\n",
    "    log_file.write(\"\\n\")\n",
    "    \n",
    "    return information\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tool that can be called by the llm to save data to memory\n",
    "@tool\n",
    "def save_data_to_memory(data):\n",
    "    \"\"\"\n",
    "    Save data to memory that can be fetched later to generate a response to the user. \n",
    "    call this function when you want to save important data like user preferences, user instructions, user personal information.\n",
    "    data: any string you want to save to memory. please format it in a way that it can be easily fetched later.\n",
    "    \"\"\"\n",
    "    #write to log file\n",
    "    log_file.write(\"Data saved: \" + data + \"\\n\")\n",
    "    log_file.write(\"\\n\")\n",
    "    \n",
    "    add_to_index(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used after the session ends to save the chat to memory for future sessions. This is not done by the llm but automatically after each session\n",
    "def save_chat_to_memory(chat):\n",
    "    if save_chat_to_memory_method == \"whole_chat\":\n",
    "        add_to_index_wholechat(chat)\n",
    "    elif save_chat_to_memory_method == \"individual_messages\":\n",
    "        add_to_index_individual_messages(chat)\n",
    "    elif save_chat_to_memory_method == \"pairs\":\n",
    "        add_to_index_pairs(chat)\n",
    "    else:\n",
    "        print(\"Invalid save_chat_to_memory_method: \" + save_chat_to_memory_method)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #without tools\n",
    "# llm = ChatOllama(\n",
    "#     model=\"mistral\",\n",
    "#     temperature=0,\n",
    "# )\n",
    "\n",
    "# #with tools\n",
    "# llm = ChatOllama(\n",
    "#     model=\"mistral\",\n",
    "#     temperature=0,\n",
    "# ).bind_tools([fetch_From_Memory, save_data_to_memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the chat so it can be put into the memory after the session ends\n",
    "chat_messages = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### TESTING WITH SOME RANDOM MEMORY DATA ###\n",
    "############################################\n",
    "\n",
    "user_data = \"name: Jordy, age: 21, very enthousiastic, prefers reading practice questions, favourite colour: green\"\n",
    "user_instructions = \"jordy wants to practice translating sentences about school from chinese to english, he wants you to give him feedback on his translations and also provice a new sentence after each feedback\"\n",
    "save_data_to_memory(user_data)\n",
    "save_data_to_memory(user_instructions)\n",
    "\n",
    "user_data = \"name: John, age: 25, very calm, prefers reading books\"\n",
    "user_instructions = \"john wants to practice translating sentences about animals from chinese to english, he wants to have conversations with you in chinese, his level is quite advanced, he has studied for 3 years already, last conversation he has held a conversation with you in chinese about studying abroad\"\n",
    "save_data_to_memory(user_data)\n",
    "save_data_to_memory(user_instructions)\n",
    "\n",
    "user_data = \"name: 玛丽, age: 30, very energetic, prefers reading stories\"\n",
    "user_instructions = \"玛丽 is a chinese teacher, she is preparing for a class about tones, she wants you to make example sentences about the weather specifying the tones of each character\"\n",
    "save_data_to_memory(user_data)\n",
    "save_data_to_memory(user_instructions)\n",
    "\n",
    "############################################\n",
    "### TESTING WITH SOME RANDOM MEMORY DATA ###\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #send a message to the llm\n",
    "# def send_message(message):\n",
    "#     messages = [\n",
    "#     (\"system\", \"\"\"YOU ARE A CONVERSATIONAL AGENT WHOSE TASK IT IS TO HELP PEOPLE WITH STUDYING LANGUAGES. \n",
    "#      YOUR NAME IS AILLP.\n",
    "#       YOU HAVE ACCESS TO A MEMORY THAT YOU CAN USE TO FETCH FROM PAST SESSIONS AND WRITE USER INFO TO REMEMBER.\n",
    "#       THE CONTENTS OF THE MEMORY IS ALL FROM THE USER AND THEIR PREFERENCES OR THEIR INSTRUCTIONS OR THEIR INFO OR THEIR PAST SESSIONS WITH YOU.\n",
    "#       EACH TIME YOU START TALKING TO A NEW USER, YOU SHOULD FETCH FROM THE MEMORY TO SEE IF YOU HAVE ANY INFO ABOUT THE USER SUCH AS THEIR NAME, AGE, PREFERENCES, INSTRUCTIONS, OR PAST SESSIONS.\n",
    "#       DON'T MENTION ANYTHING ABOUT THE TOOLS OR YOUR MEMORY TO THE USER.\n",
    "#       IF YOU DON'T USE YOUR MEMORY THEN JUST ANSWER THE USERS QUESTION WITHOUT SAYING ANYTHING ABOUT THE MEMORY.\"\"\"),\n",
    "#     (\"user\", message),\n",
    "#     ]\n",
    "#     chat_messages.append(messages)\n",
    "#     response = llm.invoke(messages)\n",
    "#     chat_messages.append(response)\n",
    "#     return response\n",
    "\n",
    "# #stream output from llm (TODO LATER)\n",
    "# def stream_messages(messages):\n",
    "#     messages = [\n",
    "#     (\"system\", \"if needed you can try to fetch from memory using the tools provided. It is not necessary to use the tools. DON'T MENTION ANYTHING ABOUT THE TOOLS TO THE USER.\"),\n",
    "#     (\"user\", \"Return the words Hello World!\"),\n",
    "#     ]\n",
    "#     for chunk in llm.stream(messages):\n",
    "#         print(chunk.text(), end=\"\")\n",
    "    \n",
    "\n",
    "# #testing phase \n",
    "# response = send_message(\"hey, im John, i want to practice chinese, can we continue from where we left of last time?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#(fetch_From_Memory, {\"query\": \"John\"})\n",
    "\n",
    "print(fetch_From_Memory(\"John\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use langchain agents for an integration of tool calling into the llm\n",
    "agents_llm = ChatOllama(model=\"llama3.1\", temperature=1.0)\n",
    "\n",
    "llm_with_tools = agents_llm.bind_tools([fetch_From_Memory, save_data_to_memory])\n",
    "\n",
    "tools = [fetch_From_Memory, save_data_to_memory]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\", \"\"\"YOU ARE A CONVERSATIONAL AGENT WHOSE TASK IT IS TO HELP PEOPLE WITH STUDYING LANGUAGES. \n",
    "     \n",
    "    #  YOUR NAME IS AILLP.\n",
    "     \n",
    "    #   YOU HAVE ACCESS TO A MEMORY THAT YOU CAN USE TO FETCH FROM PAST SESSIONS AND WRITE USER INFO TO REMEMBER.\n",
    "    #   THE CONTENTS OF THE MEMORY IS ALL FROM THE USER AND THEIR PREFERENCES OR THEIR INSTRUCTIONS OR THEIR INFO OR THEIR PAST SESSIONS WITH YOU.\n",
    "    #   EACH TIME YOU START TALKING TO A NEW USER, YOU ABSOLUTELY MUST FETCH FROM THE MEMORY TO SEE IF YOU HAVE ANY INFO ABOUT THE USER SUCH AS THEIR NAME, AGE, PREFERENCES, INSTRUCTIONS, OR PAST SESSIONS.\n",
    "    #   FOR EXAMPLE YOU CAN FETCH \\\"NAME EVA USER PREFERENCES\\\" TO GET THE USER PREFERENCES OF EVA.\n",
    "    #   GIVE A BRIEF RESPONSE TO THE USER.\n",
    "    #   IF YOU DON'T USE YOUR MEMORY THEN JUST ANSWER THE USERS QUESTION WITHOUT SAYING ANYTHING ABOUT THE MEMORY.\"\"\"),\n",
    "    #(\"system\", \"if needed you can try to fetch from memory using the tools provided. It is not necessary to use the tools.\"),\n",
    "    #(\"system\", \"you have to first read the users message!. Then if you know how to answer and in what way you have to answer, you can answer the user. If you don't know how to answer or in what way you have to use the tools to fetch from your memory by providing a query to the fetch_From_Memory tool. Then wait for the response of the tool and then you can answer the user. do not mention anything about using your tools to the user.\"),\n",
    "    (\"system\", \"\"\"YOU ARE A CONVERSATIONAL AGENT WHO HELPS USERS WITH LANGUAGE LEARNING. \n",
    "     YOUR NAME IS AILLP.\n",
    "     YOU HAVE ACCESS TO A MEMORY TOOL TO FETCH PAST USER SESSIONS AND PREFERENCES.\n",
    "     \n",
    "     AT THE START OF EVERY CONVERSATION, YOU MUST CALL THE `fetch_From_Memory` TOOL USING THE USER'S NAME.\n",
    "     YOU MUST CALL THIS TOOL BEFORE RESPONDING.\n",
    "     \n",
    "     NEVER GUESS PAST SESSIONS. ALWAYS FETCH MEMORY FIRST.\n",
    "     IF MEMORY IS FOUND, SUMMARIZE IT BEFORE ASKING THE USER HOW THEY WANT TO CONTINUE.\n",
    "     IF NO MEMORY IS FOUND, CONTINUE AS USUAL WITHOUT MENTIONING MEMORY.\n",
    "\n",
    "     NEVER MENTION THAT YOU ARE USING A TOOL TO THE USER.\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm_with_tools, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "user_input = \"hey, im John\"\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Execute the agent\n",
    "result = agent_executor.invoke({\"input\": user_input})\n",
    "\n",
    "# Final response\n",
    "print(\"\\nFinal Response:\", result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
